#!/bin/env Rscript

# clean environment before every run
rm(list=ls())

# set working directory to the path of the current script
if( rstudioapi::isAvailable() ) {
  setwd(dirname(rstudioapi::getSourceEditorContext()$path))
}

###WORKSHOP OBSERVATIONS 
# WE WILL WORK WITH CRU DATASETS BUT THESE ARE GENERIC SCRIPTS  
# Load libraries that we need to use for our script 

library(ncdf4)
library(plotrix)

# Function to generate an array containing the area of each grid cell given an
# array of longitudes and an array of latitudes. This function only works on a
# global grid and not on a regional grid as it assumes the top and bottom rows
# are half grids.
get_area <- function(lon,lat) {
  nlat <- dim(lat)
  nlon <- dim(lon)
  
  area <- array(0.0 , c(nlon, nlat))
  
  rad <- pi / 180
  
  reslat <- 180 / (nlat - 1)
  reslon <- 360 / nlon
  
  for ( i in 1:nlon ) {
    area[i,] <- abs(reslon*rad * ( sin( (lat+reslat/2.0) * rad ) - sin( ( lat - reslat/2.0 ) * rad ) ) )
    area[i,1] <- abs(reslon*rad * ( sin( lat[1] * rad ) - sin( ( lat[1] - reslat/2.0 ) * rad ) ) )
    area[i,nlat] <- area[i,1]
  }
  
  # you can test the code by adding all values of the array and comparing it
  # to the area of a sphere. The two values should be identical.
  #cat("Area of model grid = ", sum(get_area(lon,lat)) , "\n" )
  #cat("Area of a sphere = ", 4*pi, "\n"  )
  
  return(area)
}

# function to calculate the area weighted average of a given array of values
# and a array of areas as generated by the get_area function above. This assume
# a 3 dimensional array of dimensions (nlon,nlat,ntime) for the data, and a 2
# dimensinal array of dimensions (nlon,nlat) for the areas.
weightedmean <- function(mydata,myarea) {
  value <- sum( apply(mydata, c(1,2), mean, na.rm=TRUE) * myarea, na.rm=TRUE) / sum(myarea)
  
  return(value)
}

###LOAD OBSERVATIONS

inputfile_ref <- "cru_ts4.04.1901.2019.tmp.dat-regrid192x288.nc"
inputfile_mod <- "tas_Amon_CESM1-CAM5_historical_r1i1p1_185001-200512.nc"

# Open netcdf file and load into memory
ref_IN <- nc_open(inputfile_ref)
mod_IN <- nc_open(inputfile_mod)

# store dimension in own variable
lon    <- ncvar_get(ref_IN, "lon")
lat    <- ncvar_get(ref_IN, "lat")
time_ref   <- ncvar_get(ref_IN, "time")
time_mod   <- ncvar_get(mod_IN, "time")

nx  <- dim(lon)
ny  <- dim(lat)
nt_ref  <- dim(time_ref)
nt_mod  <- dim(time_mod)

# Now store the actual data we are interested in: the tmp variable
# use ncdump -h file.nc to find out which one it is.
# This is the correct variable name for the CRU file.

message("Acquiring reference temperature data...")
temp_ref <- ncvar_get(ref_IN, "tmp")

# close the file as all is in memory
nc_close(ref_IN)

message("Acquiring model temperature data...")
# model temperature is stored in Kelvin so we convert to Celsius
temp_mod <- ncvar_get(mod_IN, "tas") - 273.15

#object.size(temp_ref)
#object.size(temp_mod)

# close the file as all is in memory
nc_close(mod_IN)

area <- get_area(lon,lat)

# Here for information, we are going to compare the global weighted and non
# weighted average surface temperature for the modern time period:
cat( "Global non-area weighted mean temperatures: ", mean(temp_mod, na.rm = TRUE), "\n")
cat( "Global area weighted mean temperatures: ", weightedmean(temp_mod, area), "\n")

### GREENLAND
# between 59N and 83N, and 11W and 74W
# orginally the obs data had longitude from -180 to 180 and the model data from
# 0 to 360. After regrid, both have the longitude from 0 to 360 so we use a
# slightly difference code from workshop 5 to extract the greenland data
message("Extracting Greenland data...")
greenlandLatIdx <- which(lat > 59 & lat < 83)
greenlandLonIdx <- which(lon > 360-74 & lon < 360-11)

temp_greenland_mod <- temp_mod[greenlandLonIdx, greenlandLatIdx, ]
temp_greenland_ref <- temp_ref[greenlandLonIdx, greenlandLatIdx, ]
area_greenland     <- area[greenlandLonIdx, greenlandLatIdx]

# 提取英国数据
ukLatIdx <- which(lat > 49 & lat < 60)
ukLonIdx <- which(lon > 360-3 | lon < 2)  # 注意这里处理跨越0度经线的情况

temp_uk_mod <- temp_mod[ukLonIdx, ukLatIdx, ]
temp_uk_ref <- temp_ref[ukLonIdx, ukLatIdx, ]
area_uk     <- area[ukLonIdx, ukLatIdx]
#greenlandLonIdx <- c( which(lon > 360-5 & lon <= 360) , which(lon >= 0 & lon < 10))

# We redefine nx and ny to the number of lat and lon coordinate of our regional
# grid
nx <- length(greenlandLonIdx)
ny <- length(greenlandLatIdx)

temp_greenland_mod <- temp_mod[greenlandLonIdx, greenlandLatIdx, ]
temp_greenland_ref <- temp_ref[greenlandLonIdx, greenlandLatIdx, ]
area_greenland     <- area[greenlandLonIdx, greenlandLatIdx]

# need explaination for the date range. arrays in R start at 1 not 0 so there
# is a trick to use to get the right info. Still not sure I am getting the
# correct interval yet but if not, it is an off by one error.

message("Storing 1970-1991 temperature data...")
# Note that the code says 1992 and not 1991. This is on purpose to include the
# data for the last year
message("observation data...")
temp_ref_pres <- temp_greenland_ref[ , , seq((1970-1901)*12+1, (1992-1901)*12)]

message("model data")
temp_mod_pres <- temp_greenland_mod[ , , seq((1970-1850)*12+1, (1992-1850)*12)]

# remove the full data from memory so save resources
# to save more resources, one can change the order in which we load the
# data,and organise things on a per dataset basis instead of the per function
# basis
message("clean full data from memory")
rm(temp_ref) # > 600 MiB
rm(temp_mod) # > 800 MiB


# Calculating RMSE and Correlation

# number of cells in each dataset
N <- nx * ny * (1992-1970)*12

# Here for information, we are going to compare the global weighted and non
# weighted average surface temperature for the modern time period:
cat( "model Greenland non-area weighted mean temperatures: ", mean(temp_mod_pres, na.rm = TRUE), "\n")
cat( "model Greenland area weighted mean temperatures: ", weightedmean(temp_mod_pres, area_greenland), "\n")
cat( "observation Greenland non-area weighted mean temperatures: ", mean(temp_ref_pres, na.rm = TRUE), "\n")
cat( "observation Greenland area weighted mean temperatures: ", weightedmean(temp_ref_pres, area_greenland), "\n")

# calculating error/anomaly for each dataset
err_mod_pres <- temp_mod_pres - weightedmean(temp_mod_pres, area_greenland)
err_ref_pres <- temp_ref_pres - weightedmean(temp_ref_pres, area_greenland)

# Calculating RMSE
message("Calculating RMSE")
rmse <- sqrt ( sum( ( err_ref_pres - err_mod_pres ) ^2, na.rm=TRUE) / N ) 

# Standard deviations for each dataset
std_mod <- sqrt ( sum( err_mod_pres ^ 2, na.rm=TRUE) / N )
std_ref <- sqrt ( sum( err_ref_pres ^ 2, na.rm=TRUE) / N )

# Correlation
message("Calculating correlation...")
COR = sum( err_mod_pres * err_ref_pres, na.rm=TRUE) / (N * std_mod * std_ref)

cat("RMSE: ", rmse,'\n')
cat("Correlation: ", COR, '\n')

# We are now plotting a Taylor diagram to compare the modern data to the reference data.
# the plot will be saved as a png called taylor_diag_7.png
png(file="taylor_diag_7.png")

# we convert the data in to a format that the taylor.diagram plotting tool can understand.
ref <- as.vector(temp_ref_pres)
mod1 <- as.vector(temp_mod_pres)
# if we want to compare more than one dataset to the same reference point we can add more variables here.




# vim: ts=4 sw=4 et

# 提取格林兰数据


# 泰勒图添加英国数据点
taylor.diagram(ref, temp_uk_mod, add=TRUE, col="blue", pos.cor=TRUE, normalize=TRUE)
legend("topright", legend=c("model CESM1-CAM5 Greenland", "model CESM1-CAM5 UK"), pch=19, col=c("red", "blue"))
dev.off()
